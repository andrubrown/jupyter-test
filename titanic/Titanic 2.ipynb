{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import svm, datasets\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import re\n",
    "import seaborn as sns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#df = pd.read_csv('../input/train.csv')\n",
    "#df_test = pd.read_csv('../input/test.csv')\n",
    "df = pd.read_csv('train.csv')\n",
    "df_test = pd.read_csv('test.csv')\n",
    "\n",
    "print(\"Cols: \", df.columns)\n",
    "\n",
    "print(\"Cab\", len(df[df['Cabin'].isnull()][['Cabin', 'Name' ]]))\n",
    "print(\"Emb\", len(df[df['Embarked'].isnull()][['Embarked', 'Name' ]]))\n",
    "print(\"Sex\", len(df[df['Sex'].isnull()][['Sex', 'Name' ]]))\n",
    "print(\"Age\", len(df[df['Age'].isnull()][['Age', 'Name' ]]))\n",
    "print(\"Par\", len(df[df['Parch'].isnull()][['Parch', 'Name' ]]))\n",
    "print(\"Sib\", len(df[df['SibSp'].isnull()][['SibSp', 'Name' ]]))\n",
    "print(\"Far\", len(df[df['Fare'].isnull()][['Fare', 'Name' ]]))\n",
    "print(\"Tic\", len(df[df['Ticket'].isnull()][['Ticket', 'Name' ]]))\n",
    "print(\"Pcl\", len(df[df['Pclass'].isnull()][['Pclass', 'Name' ]]))\n",
    "\n",
    "# Filling NAs\n",
    "df[\"Embarked\"] = df[\"Embarked\"].fillna('C')\n",
    "\n",
    "# Fill missing fields with columns means\n",
    "df = df.fillna(df.mean())\n",
    "df['Cabin'] = df['Cabin'].fillna('U')\n",
    "\n",
    "# Fill missing fields with columns means\n",
    "df_test = df_test.fillna(df_test.mean())\n",
    "df_test['Cabin'] = df_test['Cabin'].fillna('U')\n",
    "\n",
    "\n",
    "# Extracting numeric part from tickets and creating a new feature\n",
    "ticketnos = []\n",
    "for s in df['Ticket']:\n",
    "    ticketnos.append(''.join([n for n in s.split() if n.isdigit()]))\n",
    "df['TicketNo'] = pd.to_numeric(pd.Series(ticketnos))\n",
    "df['TicketNo'] = df['TicketNo'].fillna(df['TicketNo'].median())\n",
    "\n",
    "ticketnos = []\n",
    "for s in df_test['Ticket']:\n",
    "    ticketnos.append(''.join([n for n in s.split() if n.isdigit()]))\n",
    "df_test['TicketNo'] = pd.to_numeric(pd.Series(ticketnos))\n",
    "\n",
    "\n",
    "print(df.describe())\n",
    "print(df.dtypes)\n",
    "\n",
    "# Transforming cabin code to a deck, adding 'U' (unknown) for the missing ones\n",
    "df['Deck'] = pd.Series([re.split('(\\d.*)',s)[0][0] for s in df['Cabin']])\n",
    "df_test['Deck'] = pd.Series([re.split('(\\d.*)',s)[0][0] for s in df_test['Cabin']])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding new features\n",
    "\n",
    "Here I try to add some more features in order to take the most from this small dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#--------------------\n",
    "# Under-18 feature\n",
    "df['U18'] = df['Age'] < 18\n",
    "df_test['U18'] = df_test['Age'] < 18\n",
    "\n",
    "bins = [0, 18, 23, 55, 80]\n",
    "df['AgeGroup'] = pd.cut(df['Age'], bins)\n",
    "df_test['AgeGroup'] = pd.cut(df_test['Age'], bins)\n",
    "\n",
    "sns.factorplot(x=\"AgeGroup\", y=\"Survived\", data=df)\n",
    "print(df[\"AgeGroup\"].unique())\n",
    "\n",
    "#--------------------\n",
    "# Family size\n",
    "df['FamilySize'] = (df['Parch'] + df['SibSp'])\n",
    "df_test['FamilySize'] = (df_test['Parch'] + df_test['SibSp'])\n",
    "\n",
    "bins = [-1, 2, 5, 7, 11]\n",
    "df['FamilySizeGroup'] = pd.cut(df['FamilySize'], bins)\n",
    "df_test['FamilySizeGroup'] = pd.cut(df_test['FamilySize'], bins)\n",
    "\n",
    "sns.factorplot(x=\"FamilySizeGroup\", y=\"Survived\", data=df)\n",
    "print(df[\"FamilySizeGroup\"].unique())\n",
    "\n",
    "#--------------------\n",
    "# Name length\n",
    "df['NameLen'] = [len(n) for n in df['Name']]\n",
    "df_test['NameLen'] = [len(n) for n in df_test['Name']]\n",
    "\n",
    "bins = [0, 20, 40, 57, 85]\n",
    "df['NameLenGroup'] = pd.cut(df['NameLen'], bins)\n",
    "df_test['NameLenGroup'] = pd.cut(df_test['NameLen'], bins)\n",
    "\n",
    "sns.factorplot(x=\"NameLenGroup\", y=\"Survived\", data=df)\n",
    "print(df[\"NameLenGroup\"].unique())\n",
    "#--------------------\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Titles\n",
    "Let's extract a feature representing passengers' titles to see how it influences the survival rate.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "titles = ['Mr.', 'Mrs.', 'Miss.', 'Master.', 'Dr.', 'Col.', 'Capt.', 'Sir.', 'Lady.', 'Countess.', 'Dona.'\n",
    "          , 'Major.', 'Don.', 'Rev.', 'Father', 'Jonkheer.', 'Mlle.', 'Ms.', 'Mme.']\n",
    "\n",
    "df['Title'] = df['Name'].apply(lambda n: str(set([w for w in n.split()]) & set(titles)) )\n",
    "df_test['Title'] = df_test['Name'].apply(lambda n: str(set([w for w in n.split()]) & set(titles)) )\n",
    "\n",
    "df['Title'].unique()\n",
    "df_test['Title'].unique()\n",
    "\n",
    "#df['Name'][df['Title']=='set()']\n",
    "#df_test['Name'][df_test['Title']=='set()']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Labeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "labels = ['Sex', 'Embarked', 'Deck', 'NameLenGroup', 'FamilySizeGroup', 'AgeGroup', 'Title']\n",
    "les = {}\n",
    "\n",
    "for l in labels:\n",
    "    print('labeling ' + l)\n",
    "    les[l] = LabelEncoder()\n",
    "    #print(df[l])\n",
    "    les[l].fit(df[l].append(df_test[l]))\n",
    "    tr = les[l].transform(df[l]) \n",
    "    df.loc[:, l + '_feat'] = pd.Series(tr, index=df.index)\n",
    "\n",
    "    tr_test = les[l].transform(df_test[l]) \n",
    "    df_test.loc[:, l + '_feat'] = pd.Series(tr_test, index=df_test.index)\n",
    "\n",
    "\n",
    "#print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train = df.drop(labels, 1) \\\n",
    "    .drop('Survived', 1) \\\n",
    "    .drop('Cabin', 1) \\\n",
    "    .drop('Ticket', 1) \\\n",
    "    .drop('NameLen', 1) \\\n",
    "    .drop('Name', 1) \\\n",
    "    .drop('PassengerId', 1)\n",
    "y_train = df['Survived']\n",
    "\n",
    "X_test = df_test.drop(labels, 1) \\\n",
    "    .drop('Cabin', 1) \\\n",
    "    .drop('Ticket', 1) \\\n",
    "    .drop('NameLen', 1) \\\n",
    "    .drop('Name', 1) \\\n",
    "    .drop('PassengerId', 1)\n",
    "\n",
    "print(\"X_train shape\", X_train.shape)\n",
    "print(\"X_test  shape\", X_test.shape)\n",
    "\n",
    "#X_train.describe()\n",
    "#X_test.describe()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "full_set = X_train[:]\n",
    "full_set['Survived'] = y_train\n",
    "\n",
    "plt.title('Pearson Correlation for training set')\n",
    "sns.heatmap(full_set.astype(float).corr(),\n",
    "            linewidths=0.1,\n",
    "            vmax=1.0, \n",
    "            square=True, \n",
    "            cmap=\"PuBuGn\", \n",
    "            linecolor='w', \n",
    "            annot=False)\n",
    "\n",
    "full_set.corr()['Survived'].abs().sort_values(ascending = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'll try to remove SibSp and Parch from the training set because they seem to be very less related to the Survival than the other features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train = X_train.drop('SibSp', 1) \\\n",
    "    .drop('Parch', 1) \n",
    "\n",
    "X_test = X_test.drop('SibSp', 1) \\\n",
    "    .drop('Parch', 1) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def dummies(train, test, columns ):\n",
    "    for column in columns:\n",
    "        train[column] = train[column].apply(lambda x: str(x))\n",
    "        test[column] = test[column].apply(lambda x: str(x))\n",
    "        good_cols = [column+'_'+i for i in train[column].unique() if i in test[column].unique()]\n",
    "        train = pd.concat((train, pd.get_dummies(train[column], prefix = column)[good_cols]), axis = 1)\n",
    "        test = pd.concat((test, pd.get_dummies(test[column], prefix = column)[good_cols]), axis = 1)\n",
    "        del train[column]\n",
    "        del test[column]\n",
    "    return train, test\n",
    "X_train, X_test = dummies(X_train, X_test, columns=['Pclass'\n",
    "                                                    , 'Sex_feat'\n",
    "                                                    , 'Embarked_feat'\n",
    "                                                    , 'Deck_feat'\n",
    "                                                    , 'TicketNo'\n",
    "                                                    , 'Title_feat'\n",
    "                                                    , 'AgeGroup_feat'\n",
    "                                                    , 'FamilySizeGroup_feat'\n",
    "                                                    , 'NameLenGroup_feat'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "full_set = X_train[:]\n",
    "full_set['Survived'] = y_train\n",
    "\n",
    "full_set.corr()['Survived'].abs().sort_values(ascending = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Random Forest Classifier\n",
    "\n",
    "I split the set in a training set and a validation set in order to check the algorithm score once trained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "import math\n",
    "\n",
    "X_tr, X_ts, y_tr, y_ts = train_test_split(X_train, y_train, test_size=0.10, random_state=42)\n",
    "print(X_tr.shape, y_tr.shape, X_ts.shape, y_ts.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'll use GridSearchCV to achieve the best set of parameters to run the final training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "forest = RandomForestClassifier(max_features='auto', oob_score=True, random_state=1, n_jobs=-1)\n",
    "param_grid = { \"criterion\" : [\"gini\", \"entropy\"]\n",
    "              , \"min_samples_leaf\" : [1, 5, 10]\n",
    "              , \"min_samples_split\" : [2, 4, 10, 12, 16]\n",
    "              , \"n_estimators\": [25, 50, 100, 400, 700]}\n",
    "gs = GridSearchCV(estimator=forest, param_grid=param_grid, scoring='accuracy', cv=3, n_jobs=-1)\n",
    "gs = gs.fit(X_tr, y_tr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(gs.best_score_)\n",
    "print(gs.best_params_)\n",
    "#print(gs.cv_results_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier( criterion='entropy', \n",
    "                             n_estimators=400,\n",
    "                             min_samples_split=16,\n",
    "                             min_samples_leaf=1,\n",
    "                             max_features='auto',\n",
    "                             oob_score=True,\n",
    "                             random_state=1,\n",
    "                             n_jobs=-1)\n",
    "\n",
    "rf.fit(X_tr, y_tr)\n",
    "pred = rf.predict(X_ts)\n",
    "\n",
    "score = rf.score(X_ts, y_ts)\n",
    "err = math.sqrt(((pred - y_ts)**2).mean())\n",
    "print(\"Error: %.3f Score: %.3f\" % (err, score))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see the features importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pd.concat((pd.DataFrame(X_train.iloc[:, 1:].columns, columns = ['variable']), \n",
    "           pd.DataFrame(rf.feature_importances_, columns = ['importance'])), \n",
    "          axis = 1).sort_values(by='importance', ascending = False)[:20]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally let's generate the submission file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Training the validated model with the whole training set\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "pred = rf.predict(X_test)\n",
    "\n",
    "df_test['Survived'] = pd.Series(pred)\n",
    "sub = df_test[['PassengerId','Survived']]\n",
    "\n",
    "sub.to_csv('submission_forest.csv', index=False)\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
